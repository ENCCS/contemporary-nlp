{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2 - Capturing more of the syntax\n",
    "\n",
    "In the previous lesson we saw how simple bag-of-words representations could be used to find similar documents. While it works somewhat well to find relevant documents, the model we use has a very simple representation of language, where all meaning derived from syntax are lost. We'll now look at how can use pre-trained neural networks to get representations of text which capture some of this syntax.\n",
    "\n",
    "Today, this is most often done by using _Transformer_ neural networks pre-trained with _language modelling_. Essentially, the pretraining task is framed as learning the joint distribution over text by estimating the factorized distribution. This can be done in many ways (e.g. GPT, BERT, XLNet).\n",
    "\n",
    "It has been noted that this pre-training task works well when later fine-tuning on some supervised task. In our case though, we would like to use some representation of the documents for similarity search, without doing any additional fine tuning.\n",
    "\n",
    "To do this, we will use _sentence BERT_ (sBERT), a variant of the BERT training procedure which strives to improve performance of the model for semantic representations.\n",
    "\n",
    "## Huggingface Transformers\n",
    "\n",
    "Much of the community surrounding pre-trained language models has centered on a project named Hugginface Transformers. This started as a library of basic Transformer models (in particular including pretrained BERT and GPT models), but has grown to be a substantial platform for pre-trained models.\n",
    "\n",
    "Huggingface makes working with these models simple, and hides much of the inner workings behind an easy to use interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "data_url = \"https://cdn.thingiverse.com/assets/d0/b3/68/63/1e/Gate_Guide_Spacer_v9.stl\"\n",
    "data_root = Path('data')\n",
    "data_path = data_root / 'sampled_archive.zip'\n",
    "data_root.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "class ZipPatentCorpus:\n",
    "    def __init__(self, *, document_archive: Path, document_parts=('abstract', 'description', 'claims'), lang='en'):\n",
    "        self.document_archive = document_archive\n",
    "        self.document_zf = zipfile.ZipFile(self.document_archive)\n",
    "        self.document_parts = document_parts\n",
    "        self.lang = lang\n",
    "\n",
    "        self.documents  = sorted(filename for filename in self.document_zf.namelist())\n",
    "        self.symbolic_labels = []\n",
    "        self.labeled_documents = defaultdict(list)\n",
    "        for document in self.documents:\n",
    "            label, sep, file = document.rpartition('/')\n",
    "            self.symbolic_labels.append(label)\n",
    "            self.labeled_documents[label].append(document)\n",
    "        self.label_codes = {label: i for i, label in enumerate(sorted(self.labeled_documents.keys()))}\n",
    "        self.labels = [self.label_codes[label] for label in self.symbolic_labels]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "    def load_document(self, document_path):\n",
    "        with self.document_zf.open(document_path) as fp:\n",
    "            document = json.load(fp)\n",
    "            document_str = '\\n'.join([document[part][self.lang] for part in self.document_parts])\n",
    "            return document_str\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Lazily load documents here\n",
    "        if isinstance(item, slice):\n",
    "            document_paths = self.documents[item]\n",
    "            document_str = [self.load_document(document_path) for document_path in document_paths]\n",
    "        elif isinstance(item, Sequence):\n",
    "            document_str = [self.load_document(self.documents[idx]) for idx in item]\n",
    "        else:\n",
    "            document_str = self.load_document(self.documents[item])\n",
    "        return document_str\n",
    "    \n",
    "    def get_label(self, i):\n",
    "        return self.labels[i]\n",
    "    \n",
    "    def get_symbolic_label(self, i):\n",
    "        return self.symbolic_labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, \n",
    "                 *, \n",
    "                 max_vocab_size, \n",
    "                 stoplist=('the', 'of', 'a', 'and', 'to', 'in', 'is', 'or', 'an', 'by', 'as', 'be', 'for'),\n",
    "                 wordpattern=r\"[A-Za-z0-9\\-\\+='.]*[A-Za-z][A-Za-z0-9\\-\\+='.]*\"\n",
    "                 ):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.stoplist = stoplist\n",
    "        self.wordpattern = re.compile(wordpattern)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [word.strip('.') for word in re.findall(self.wordpattern, text.lower())]\n",
    "\n",
    "    def encode(self, tokenized_text):\n",
    "        try:\n",
    "            term_to_index = self.term_to_index\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"Tokenizer is missing term to index, did you call Tokenizer.fit() or Tokenizer.fit_transform()?\")\n",
    "        return [term_to_index[term] for term in tokenized_text if term in term_to_index]\n",
    "    \n",
    "    def decode(self, encoded_text):\n",
    "        try:\n",
    "            index_to_term = self.index_to_term\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"Tokenizer is missing term to index, did you call Tokenizer.fit() or Tokenizer.fit_transform()?\")\n",
    "\n",
    "        return [index_to_term[idx] for idx in encoded_text]\n",
    "    \n",
    "    def make_vocab(self, documents_term_frequencies):\n",
    "        document_occurance_counts = Counter()\n",
    "        for document_term_frequency in documents_term_frequencies:\n",
    "            # And a count once for each unique term in a document\n",
    "            document_occurance_counts.update(document_term_frequency.keys()) \n",
    "        \n",
    "        for stopword in self.stoplist:\n",
    "            del document_occurance_counts[stopword]\n",
    "        \n",
    "        self.vocabulary = sorted(term for term, count in document_occurance_counts.most_common(self.max_vocab_size) if count > 1)\n",
    "        self.term_to_index = {term: i for i, term in enumerate(self.vocabulary)}\n",
    "        self.index_to_term = {i: term for term, i in self.term_to_index.items()}\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        documents_term_frequencies = [Counter(self.tokenize(doc)) for doc in tqdm(corpus, desc=\"Tokenizing\", leave=False)]\n",
    "        self.make_vocab(documents_term_frequencies)\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        tokenized_docs = [self.tokenize(doc) for doc in tqdm(corpus, desc=\"Tokenizing\", leave=False)]\n",
    "        documents_term_frequencies = [Counter(tokens) for tokens in tokenized_docs]\n",
    "        self.make_vocab(documents_term_frequencies)\n",
    "        return [self.encode(tokenized_text) for tokenized_text in tqdm(tokenized_docs, desc=\"Encoding\", leave=False)]\n",
    "\n",
    "    def transform(self, text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        encoded_text = self.encode(tokenized_text)\n",
    "        return encoded_text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "class NGramTokenizer(Tokenizer):\n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 n,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        self.n = n\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def fit(self, corpus):\n",
    "        documents_term_frequencies = []\n",
    "        for doc in tqdm(corpus, desc=\"Tokenizing\", leave=False):\n",
    "            tokenized = self.tokenize(doc)\n",
    "            if not self.include_stop_ngrams:\n",
    "                tokenized = [token for token in tokenized if token not in self.stoplist]\n",
    "            document_terms = Counter(tokenized)\n",
    "            \n",
    "            for n in range(1, self.n):  # note that since we use the n in the slice below, for 2-grams we want this offset to be 1 and so on\n",
    "                n_grams = [' '.join(tokenized[i:i+n]) for i in range(len(tokenized))-1]\n",
    "                document_terms.update(n_grams)\n",
    "            documents_term_frequencies.append(document_terms)\n",
    "        self.make_vocab(documents_term_frequencies)\n",
    "            \n",
    "        \n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        tokenized_docs = []\n",
    "        documents_term_frequencies = []\n",
    "        for doc in tqdm(corpus, desc=\"Tokenizing\", leave=False):\n",
    "            tokenized = self.tokenize(doc)\n",
    "            if not self.include_stop_ngrams:\n",
    "                tokenized = [token for token in tokenized if token not in self.stoplist]\n",
    "            document_terms = Counter(tokenized)\n",
    "            \n",
    "            for n in range(1, self.n):  # note that since we use the n in the slice below, for 2-grams we want this offset to be 1 and so on\n",
    "                n_grams = [' '.join(tokenized[i:i+n]) for i in range(len(tokenized))-1]\n",
    "                document_terms.update(n_grams)\n",
    "            documents_term_frequencies.append(document_terms)\n",
    "        self.make_vocab(documents_term_frequencies)\n",
    "        \n",
    "        tokenized_docs = [self.tokenize(doc) for doc in tqdm(corpus, desc=\"Tokenizing\", leave=False)]\n",
    "        documents_term_frequencies = [Counter(tokens) for tokens in tokenized_docs]\n",
    "        self.make_vocab(documents_term_frequencies)\n",
    "        return [self.encode(tokenized_text) for tokenized_text in tqdm(tokenized_docs, desc=\"Encoding\", leave=False)]\n",
    "\n",
    "    def transform(self, text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        encoded_text = self.encode(tokenized_text)\n",
    "        return encoded_text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = ZipPatentCorpus(document_archive=data_path, document_parts=['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(max_vocab_size=100000)\n",
    "tokenized_docs = tokenizer.fit_transform(text_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'transformers' from 'F:\\\\Anaconda\\\\envs\\\\enccs-nlp-workshop\\\\lib\\\\site-packages\\\\transformers\\\\__init__.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_38868\\1401220064.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"This is an example sentence\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Each sentence is converted\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AI-Growth-Lab/PatentSBERTa'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\enccs-nlp-workshop\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_name_or_path, modules, device, cache_folder)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'modules.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m#Load as SentenceTransformer model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                 \u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_sbert_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m#Load with AutoModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_auto_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\enccs-nlp-workshop\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36m_load_sbert_model\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule_config\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules_config\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[0mmodule_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_from_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m             \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodule_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\enccs-nlp-workshop\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(input_path)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msbert_config_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfIn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfIn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\enccs-nlp-workshop\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer_name_or_path\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtokenizer_name_or_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtokenizer_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\enccs-nlp-workshop\\lib\\site-packages\\transformers\\utils\\dummy_pt_objects.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m         \u001b[0mrequires_backends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"torch\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\enccs-nlp-workshop\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__name__\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBACKENDS_MAPPING\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBACKENDS_MAPPING\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('AI-Growth-Lab/PatentSBERTa', device=device)\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bac2398dd58d2031b48b8e6629ce0629f95a909353e651ad60e56e132652e41e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('enccs-nlp-workshop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
